# Jesse the (ro)bot ğŸ¤ ğŸ¤–ğŸ§¹
*A robotic vacuum cleaner simulation powered by Reinforcement Learning.*

![cover](images/cover.png)

## ğŸš€ About
This repository contains the implementation of **Jesse the (ro)bot**, a simulated robotic vacuum cleaner trained with Reinforcement Learning as part of the course *Complex Systems: Models and Simulation* at UniversitÃ  degli studi di Milano-Bicocca (Italy).  

The goal was not to deliver a finished industrial product, but to explore RL in practice, design a realistic environment, and see how an agent can learn to clean, avoid collisions, and return to its base â€” starting from zero knowledge.

## ğŸ§  Main Features
- Continuous 2D environment + discrete occupancy grid abstraction.
- Robot equipped with LiDAR and collision sensors.
- **Two RL agents implemented**:  
  - Q-learning (baseline)  
  - PPO (single-MLP and multi-encoder)  
- Curriculum learning with maps of increasing difficulty.  
- Reward shaping for exploration, cleaning, and safe return.  
- Modular PPO multi-encoder architecture:  
  - 2D CNN for local occupancy patch  
  - 1D CNN for LiDAR encoder  
  - Side-views encoder  
  - Scalars encoder  

## ğŸ“Š Results
- The PPO multi-encoder agent developed systematic exploration and cleaning behaviors.  
- Training on CPU-only hardware was extremely slow: each meaningful modification required some times of training just to observe its effect.  
- Despite this, the project demonstrated how RL, careful reward shaping, and structured state encoding can produce meaningful autonomous behavior.

## ğŸ”® Future Work
- Improve reward shaping (the most difficult challenge so far).  
- Continue training on harder maps (requires GPU/HPC resources).  
- Experiment with network architectures (deeper CNNs, attention).  
- Add noise to motion dynamics and sensors for realism.  
- Introduce static and dynamic obstacles (e.g., furniture, pets ğŸ±).  
- Extend the battery model with orientation/acceleration costs.  

## ğŸ“‚ Repository Structure
```
Jesse-the-Robot-Vacuum-Cleaner
â”‚   README.md
â”‚   report.pdf
â”‚   cover.png
â”‚
â””â”€â”€â”€code
    â”‚   play.py                # entry point to run the simulation
    â”‚
    â””â”€â”€â”€rlrc
        â”‚   joystick.py        # optional joystick controller
        â”‚   test.py            # evaluation script
        â”‚   train.py           # training script (PPO with curriculum learning)
        â”‚   utils.py           # helper functions (plotting, metrics, etc.)
        â”‚   __init__.py
        â”‚
        â”œâ”€â”€â”€classes
        â”‚   â”‚   agent.py       # Q-learning and PPO agents (you have to switch between branches)
        â”‚   â”‚   encoders.py    # CNN/MLP encoders for multi-modal inputs
        â”‚   â”‚   environment.py # continuous environment simulation
        â”‚   â”‚   graphics.py    # rendering and visualization
        â”‚   â”‚   robot.py       # robot dynamics, sensors, reward shaping
        â”‚
        â””â”€â”€â”€constants
            â”‚   colors.py
            â”‚   configuration.py
            â”‚   maps.py        # predefined training/test maps
```

## ğŸ“– Documentation
For a complete description of the project, see the full report:  
ğŸ‘‰ [Report PDF](./report.pdf)

---

*Created by Alberto Sormani (2025) as part of one of the courses of the Master's Degree in Computer Science at the University of Milan-Bicocca.*
